{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "local-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# env.reset()\n",
    "# for _ in range(500):\n",
    "#     env.render()\n",
    "#     env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, n_states, n_actions, lr, gamma, ep, min_epsilon,ep_decay):\n",
    "        \n",
    "        \"\"\"\n",
    "        parameters\n",
    "        \"\"\"\n",
    "        # environment\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # parameters\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.epsilon = ep  # exploration rate\n",
    "        self.epsilon_min = min_epsilon\n",
    "        self.epsilon_decay = ep_decay\n",
    "        self.learning_rate = lr\n",
    "        self.memory = collections.deque(maxlen =50000)\n",
    "        self.replay_count = 0\n",
    "        \n",
    "        # local network and target network\n",
    "        self.q_local = self.build_dqn()\n",
    "        self.q_target = self.build_dqn()\n",
    "\n",
    "        \n",
    "  \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def update(self):\n",
    "     # print(\"update target network\")\n",
    "      self.q_target.set_weights(self.q_local.get_weights())\n",
    "  \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        return np.argmax(self.q_local.predict(state)[0])\n",
    "    \n",
    "    def select_action_test(self, state):\n",
    "        return np.argmax(self.q_local.predict(state)[0])\n",
    "    \n",
    "    def replay(self, batch_size,step):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        #target_f = self.model.predict_on_batch(batch[0])\n",
    "        \n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        next_states = []\n",
    "        rewards = []\n",
    "        finishes = []\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            finishes.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        next_states = np.array(next_states)\n",
    "        rewards = np.array(rewards)\n",
    "        finishes = np.array(finishes)\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        \n",
    "        q_vals_next_state = self.q_target.predict_on_batch(next_states)\n",
    "        q_vals_target = self.q_local.predict_on_batch(states)\n",
    "        \n",
    "        max_q_values_next_state = np.amax(q_vals_next_state, axis=1)\n",
    "        q_vals_target[np.arange(batch_size), actions] = rewards + self.gamma * (max_q_values_next_state) * (1 - finishes)\n",
    "        self.q_local.fit(states, q_vals_target, verbose=0)\n",
    "        self.replay_count += 1\n",
    "\n",
    "        \n",
    "        if 0<=step<=50:\n",
    "            self.epsilon = 0.5\n",
    "        elif 50<step<=100:\n",
    "            self.epsilon = 0.3\n",
    "        elif 100<step<=300:\n",
    "            self.epsilon = 0.2\n",
    "        elif 300<step<=500:\n",
    "            self.epsilon = 0.1\n",
    "        elif 500<step<=1000:\n",
    "            self.epsilon = 0.05\n",
    "        else:\n",
    "            self.epsilon = 0.01\n",
    "    \n",
    "    def build_dqn(self):      \n",
    "        \"\"\"\n",
    "        Q-function Approximator\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim = self.n_states, activation='relu'))\n",
    "        model.add(Dense(64,  activation='relu'))\n",
    "        model.add(Dense(self.n_actions, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.q_local.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "under-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 states and 4 actions in the Environment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "\n",
    "MAX_MOVES =1000\n",
    "AVG_REWARD_LEN = 100\n",
    "TARGET_AVG_REWARD = 200\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "EPSILON = 1.0\n",
    "MIN_EPSILON = 0.01\n",
    "EPSILON_DECAY = 0.05\n",
    "TARGET_UPDATE = 20\n",
    "\"\"\"\n",
    "Randomization Seeds\n",
    "\"\"\"\n",
    "random_seed = 0\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\"\"\"\n",
    "Environment\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(f\"There are {n_states} states and {n_actions} actions in the Environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "posted-lucas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward:223.66576802082602\n",
      "total reward:270.7219101002065\n",
      "total reward:276.9658156161074\n",
      "total reward:276.8789445203622\n",
      "total reward:263.2889928622239\n",
      "total reward:271.80706707277284\n",
      "total reward:239.7376037160408\n",
      "total reward:278.1044890928906\n",
      "total reward:280.94236002495734\n",
      "total reward:260.70014736150983\n"
     ]
    }
   ],
   "source": [
    "test_reward = []\n",
    "agent_test = DQNAgent(n_states, n_actions, lr = LEARNING_RATE,gamma = GAMMA, ep = EPSILON, min_epsilon = MIN_EPSILON,ep_decay = EPSILON_DECAY )\n",
    "agent_test.q_local = keras.models.load_model('dqn_defined_epsilon_update20')\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, n_states])\n",
    "    t_rewards = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = agent_test.select_action_test(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        t_rewards += reward\n",
    "        next_state = np.reshape(next_state, [1, n_states])\n",
    "        #agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"total reward:{t_rewards}\")   \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-processing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-syria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
